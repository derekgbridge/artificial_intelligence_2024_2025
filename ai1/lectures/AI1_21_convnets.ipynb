{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Convolutional Neural Networks</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgement</h1>\n",
    "<ul>\n",
    "    <li>The first image is scanned from Figure 13-1 in: A. G&eacute;ron: \n",
    "        <i>Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow (2nd edn)</i>, O'Reilly, 2019</li>\n",
    "    <li>The final image was produced by adapting the code from \n",
    "        <a href=\"https://github.com/gwding/draw_convnet\">https://github.com/gwding/draw_convnet</a>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Primate Vision</h1>\n",
    "<ul>\n",
    "    <li>In the primate vision system, there seems to be a hierarchy of neurons within the visual cortex:\n",
    "        <figure>\n",
    "            <img src=\"images/locality.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>In the lowest layers, \n",
    "        <ul>\n",
    "            <li>neurons have small local receptive fields, i.e. they respond to stimuli in a limited\n",
    "                region of the visual field; and\n",
    "            </li>\n",
    "            <li>they respond to, e.g., spots of light.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In higher layers,\n",
    "        <ul>\n",
    "            <li>they combine the outputs of neurons in the lower layers;</li>\n",
    "            <li>they have larger receptive fields; and</li>\n",
    "            <li>they respond to, e.g., lines at particular orientations.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In the highest layers,\n",
    "        <ul>\n",
    "            <li>they respond to ever more complex combinations, such as shapes and objects.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There are perhaps as many as 8 layers in the visual cortex alone:\n",
    "        <figure>\n",
    "            <img src=\"images/vision.gif\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<h1>Convolutional Neural Networks</h1>\n",
    "<ul>\n",
    "    <li>Convolutional Neural Networks (convnets) are widely used in computer vision and in other\n",
    "        perceptual problems including speech recognition and natural language processing.\n",
    "    </li>\n",
    "    <li>We will use 2D convnets, which are widely used for dataset of images.</li>\n",
    "    <li>They have nice properties, some of which resemble the visual cortex in primates:\n",
    "        <ul>\n",
    "            <li>They learn features that are <b>translation invariant</b>:\n",
    "                <ul>\n",
    "                    <li>A feature map in a convolutional layer will recognize that feature anywhere in\n",
    "                        the image: bottom-left, top-right, &hellip;\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>They learn <b>spatial hierarchies</b> of features:\n",
    "                <ul>\n",
    "                    <li>from small local features such as lines in lower layers up to larger shapes\n",
    "                        in higher layers.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MNIST Example</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This is how we <em>were</em> preprocessing the MNIST dataset. Note the flattening (reshaping):\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "# Load MNIST into four Numpy arrays\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "mnist_x_train = mnist_x_train.reshape((60000, 28 * 28))\n",
    "mnist_x_test = mnist_x_test.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>But below is what we will do from now on. Note the reshaping is now different:\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "# Load MNIST into four Numpy arrays\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "mnist_x_train = mnist_x_train.reshape((60000, 28, 28, 1))\n",
    "mnist_x_test = mnist_x_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Images are Rank 3 Tensors</h1>\n",
    "<ul>\n",
    "    <li>Grayscale images:\n",
    "        <ul>\n",
    "            <li>A grayscale image has a certain height $h$ and width $w$. Therefore, it makes sense to\n",
    "                represent them as rank 2 tensors (matrices) of integers $[0, 255]$.\n",
    "            </li>\n",
    "            <li>Up to now, however, we have reshaped them into rank 1 tensors (vectors):\n",
    "                <pre>\n",
    "mnist_x_train = mnist_x_train.reshape((60000, 28 * 28))\n",
    "                </pre>\n",
    "                <figure>\n",
    "                    <img src=\"images/reshape.png\" />\n",
    "                </figure>\n",
    "                What is the disadvantage of this: what information gets destroyed?\n",
    "            </li>\n",
    "            <li>So, henceforth, we will not flatten them in this way.</li>\n",
    "            <li>In fact, for consistency with colour images, we will treat grayscale images as rank 3 tensors\n",
    "                of shape $(h, w, 1)$\n",
    "                <pre>\n",
    "mnist_x_train = mnist_x_train.reshape((60000, 28, 28, 1))                \n",
    "                </pre>\n",
    "                <figure>\n",
    "                    <img src=\"images/grayscale.png\" />\n",
    "                </figure>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Colour images:\n",
    "        <ul>\n",
    "            <li>These will be rank 3 tensors: height $h$, width $w$, and channels (or depth) $d$.</li>\n",
    "            <li>$d = 3$. Why?\n",
    "                <figure>\n",
    "                    <img src=\"images/rgb.png\" />\n",
    "                </figure>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Datasets of images:\n",
    "        <ul>\n",
    "            <li>Datasets of images will be rank 4 tensors: $(m, h, w, d)$.</li>\n",
    "            <li>What is $m$?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Why will datasets of videos be rank 5 tensors?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Convnet for MNIST</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28, 28, 1))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(10, activation=\"softmax\")(x)\n",
    "convnet = Model(inputs, outputs)\n",
    "convnet.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Note the input shape.</li>\n",
    "    <li>Note the three numbers that configure convolutional layers:\n",
    "        <ul>\n",
    "            <li>number of feature maps (filters); and</li>\n",
    "            <li>height and width of a window (sometimes called 'convolutional kernel'), which corresponds\n",
    "                roughly to the idea of a receptive field.\n",
    "            </li>\n",
    "        </ul>\n",
    "        There may be strides and padding.\n",
    "    </li>\n",
    "    <li>Note the numbers that configure max pooling layers:\n",
    "        <ul>\n",
    "            <li>height and width of a window (sometimes called the 'pooling window').</li>\n",
    "        </ul>\n",
    "        Again there may be strides and padding.\n",
    "    </li>\n",
    "    <li>Notice the flattening, similar to the reshaping we were doing on the MNIST dataset before.\n",
    "        This enables us to have some layers at the 'top' of the network that are densely connected,\n",
    "        in the familiar way.\n",
    "    </li>\n",
    "    <li>In particular, the output layer is determined by the task: here we're doing multi-class\n",
    "        classification.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Although we don't understand it fully yet, we'll train it.</li>\n",
    "     <li>Training takes some time (unsurprising when we look at the number of parameters, above)\n",
    "        but accuracy is now even higher.\n",
    "    </li>\n",
    "    <li>Memory requirements for the network and for all the results that get stored during training are high,\n",
    "        which is one reason to reduce mini-batch size.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1548b7380>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet.fit(mnist_x_train, mnist_y_train, epochs=20, batch_size=32, \n",
    "            verbose=0, validation_split=0.2, \n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9853 - loss: 0.0442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9878000020980835"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = convnet.evaluate(mnist_x_test, mnist_y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convolutional Layers</h1>\n",
    "<ul>\n",
    "    <li>Consider a neural network whose inputs are images (each is a rank 3 tensor).</li>\n",
    "    <li>A 2D convolutional layer is a rank 3 tensor of neurons, whose shape is $(h, w, d)$:\n",
    "        <ul>\n",
    "            <li>where $d$, the depth, is the number of <b>feature maps</b></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>For simplicity to begin with, let's assume $d = 1$.</li>\n",
    "    <li>Connections:\n",
    "        <ul>\n",
    "            <li>In the case of a dense layer, we saw that every neuron in that layer has connections from\n",
    "                every neuron in the preceding layer.\n",
    "            </li>\n",
    "            <li>But in the case of a convolutional layer, every neuron in that\n",
    "                layer has connections from only a small rectangular <b>window</b> of neurons\n",
    "                in the preceding layer, typically $3 \\times 3$ or $5 \\times 5$.\n",
    "                <figure style=\"text-align: center;\">\n",
    "                    <img src=\"images/rectangles.png\" /><br />\n",
    "                    <img src=\"images/conv_S1P0.gif\" width=\"450px\" />\n",
    "                    <figcaption>\n",
    "                        Animated GIF from <a href=\"www.MLinGIFS.aqeel-anwar.com \">www.MLinGIFS.aqeel-anwar.com </a>\n",
    "                    </figcaption>\n",
    "                </figure>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convolutional layers: height and width</h2>\n",
    "<ul>\n",
    "    <li>Suppose the shape of the preceding layer is $(28, 28, 1)$ and the windows in the convolutional\n",
    "        layer are $3 \\times 3$\n",
    "    </li>\n",
    "    <li>This gives a convolutional layer whose height is 26 and whose width is 26. Why?</li>\n",
    "    <li>Extra details that you can ignore in CS4618:\n",
    "        <ul>\n",
    "            <li>In fact, if we wish, we can make the convolutional layer have the same height and width\n",
    "                as the preceding layer:\n",
    "                <ul>\n",
    "                    <li>Padding: add a border of zeros around the previous layer.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>And, if we wish, we can make the convolutional layer have even smaller height and width\n",
    "                than the preceding layer:\n",
    "                <ul>\n",
    "                    <li>Strides: instead of overlapping windows, we can introduce a distance between\n",
    "                        successive windows.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convolutional layers: the weights of a feature map</h2>\n",
    "<ul>\n",
    "    <li>Continue to assume $d=1$, the convolutional layer consists of one feature map.</li>\n",
    "    <li>The idea of a feature map is that it will learn a specific aspect (feature) of its input:\n",
    "        <ul>\n",
    "            <li>e.g. the presence of a vertical line;</li>\n",
    "            <li>e.g.. the presence of a pair of eyes.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Within one feature map, all neurons share the same weights!\n",
    "        <figure>\n",
    "            <img src=\"images/shared_weights.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Advantages:\n",
    "        <ul>\n",
    "            <li>This reduces the number of parameters that must be learned.</li>\n",
    "            <li>More importantly, it means that the feature map will respond to the presence of\n",
    "                that feature <em>no matter where it is in the input</em> (the <em>translational\n",
    "                invariance</em>, mentioned earlier).\n",
    "            </li>\n",
    "            <li>You may see the word <b>filter</b> to refer to the weights of the neurons in\n",
    "                a feature map.\n",
    "            </li>\n",
    "       </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convolutional layers: stacks of feature maps</h2>\n",
    "<ul>\n",
    "    <li>Now consider the case where $d > 1$: the convolutional layer comprises a stack of $d$ feature maps.</li>\n",
    "    <li>A neuron in a feature map in a convolutional layer is connected to a window of neurons\n",
    "        in <em>each</em> of the feature maps of the previous layer (or, in the case of the first layer, in each\n",
    "        of the channels of the input).\n",
    "        <figure>\n",
    "            <img src=\"images/hierarchy.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Note how this means that a feature map in one layer combines several feature maps (or channels) of\n",
    "        the previous layer (the <em>spatial hierarchy</em>, mentioned earlier).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pooling Layers</h1>\n",
    "<ul>\n",
    "    <li>The goal is to have a layer that shrinks the number of neurons in higher layers:\n",
    "        <ul>\n",
    "            <li>to reduce the amount of computation;</li>\n",
    "            <li>to reduce memory usage;</li>\n",
    "            <li>to reduce the number of parameters to be learned, thus reducing the risk of\n",
    "                overfitting; and\n",
    "            </li>\n",
    "            <li>to create a hierarchy in which higher convolutional layers contain information about\n",
    "                the totality of the original input image.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Again, it works on rectangular windows: neurons in the pooling layer are connected to windows\n",
    "        of neurons in the previous layer\n",
    "        <ul>\n",
    "            <li>typically $2 \\times 2$;</li>\n",
    "            <li>typically adajcent rather than overlapping.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>E.g. if the previous layer has height $h$ and width $w$, and the pooling layer uses adjacent\n",
    "        $2 \\times 2$ pooling windows, then the pooling layer will have height $h/2$ and width $w/2$.\n",
    "        <figure>\n",
    "            <img src=\"images/pooling.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>The depth of the pooling layer is the same as the depth of the previous layer.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Max pooling layers</h2>\n",
    "<ul>\n",
    "    <li>Pooling layers have no weights: nothing to learn.</li>\n",
    "    <li>In a <b>max pooling layer</b>, \n",
    "        <ul>\n",
    "            <li>a neuron in the pooling layer receives the outputs of the \n",
    "                neurons in the window \n",
    "                in the previous layer and outputs only the largest of them.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Pooling layers work on the feature maps independently, which is why they have the same depth\n",
    "        as the previous layer.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Check Your Understanding</h1>\n",
    "<ul>\n",
    "    <li>Do you understand the numbers in the code?</li>\n",
    "    <li>Do you understand the numbers in the output of <code>convnet.summary()</code>?</li>\n",
    "    <li>Do you understand the diagram below?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"images/mnist.png\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Final Remarks</h1>\n",
    "<ul>\n",
    "    <li>Note how convolutional layers are computationally efficient:\n",
    "        <ul>\n",
    "            <li>They have fewer parameters than dense layers (although, care here, because each one is involved in more multiplications).</li>\n",
    "            <li>They can be easily parallelised.</li>\n",
    "        </ul>\n",
    "        This is one reason for their popularity. \n",
    "    </li>\n",
    "    <li>Consider tasks that involve audio, text and time series data. For these tasks, Recurrent Neural Networks would be the obvious choice (see CS4619 AI2). But, in some cases, 1D convolutional networks can be used successfuly instead. (Although, to be fair, these days Transformers (also covered in CS4619 AI2) are displacing Recurrent Neural Networks and 1D convnets in many cases.)</li>\n",
    "    <li>Consider tasks that involve graphs (nodes and edges). We can process these using Graph Neural Networks. There are graph convolutions that can be used within these neural networks.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
