{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Neural Networks</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgements</h1>\n",
    "<ul>\n",
    "    <li>The colourful diagrams are my own invention but were improved by seeing similar diagrams in materials produced by Sebastian Raschka.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Introduction</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$\n",
    "<ul>\n",
    "    <li><b>Neural Networks</b> are <em>loosely</em> inspired by what we know about our brains:\n",
    "        <ul>\n",
    "            <li>Networks of neurons.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>However, they are <em>not</em> models of our brains.\n",
    "         <ul>\n",
    "             <li>E.g. there is no evidence that the brain uses the learning algorithm that is used by neural\n",
    "                 networks.\n",
    "             </li>\n",
    "         </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Biological Neurons</h1>\n",
    "<ul>\n",
    "    <li>Your brain is a network of about 10<sup>11</sup> neurons, each connected to about 10<sup>4</sup> others:\n",
    "        <figure>\n",
    "            <img src=\"images/brain.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Sufficient electrical activity on a neuronâ€™s dendrites causes an electrical pulse\n",
    "        to be sent down the axon, where it may activate other neurons.\n",
    "        <figure style=\"text-align: center\">\n",
    "            <img src=\"images/cell.png\" />\n",
    "            <figcaption>\n",
    "                <a style=\"fint-size: 0.8em\" href=\"https://commons.wikimedia.org/wiki/File:Neuron.svg\">https://commons.wikimedia.org/wiki/File:Neuron.svg</a>\n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Artificial Neurons</h1>\n",
    "<ul>\n",
    "    <li>A simple artificial neuron:\n",
    "        <figure>\n",
    "            <img src=\"images/ltu.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>It has $n$ real-valued inputs, $\\v{x}_1,\\ldots,\\v{x}_n$.</li>\n",
    "    <li>The connections have real-valued weights, $\\v{w}_1,\\ldots,\\v{w}_n$.</li>\n",
    "    <li>The neuron also has a number $b$ called the <b>bias</b>.</li>\n",
    "    <li>The neuron computes the weighted sum of its inputs and adds $b$:\n",
    "        $$z = b + \\v{w}_1\\v{x}_1 + \\v{w}_2\\v{x}_2 + \\cdots + \\v{w}_n\\v{x}_n$$\n",
    "        or if $\\v{x}$ is a row vector of the inputs and $\\v{w}$ is a (column) vector of the weights\n",
    "        $$z = b + \\v{x}\\v{w}$$\n",
    "    </li>\n",
    "    <li>The neuron then usually applies an <b>activation function</b>, $g$, to the weighted sum, $z$.\n",
    "        Many activation functions have been proposed, including:\n",
    "        <ul>\n",
    "            <li><b>linear activation function</b>: $$g(z) = z$$</li>\n",
    "            <li><b>step activation function</b>:\n",
    "                $$g(z) = \\left\\{ \\begin{array}{lr}\n",
    "                    0 & \\mbox{if } z < 0 \\\\\n",
    "                    1 & \\mbox{if } z \\geq 0\n",
    "                    \\end{array}\n",
    "                  \\right.\n",
    "                $$\n",
    "            </li>\n",
    "            <li><b>sigmoid activation function</b>: $$g(z) = \\frac{1}{1 + e^{-z}}$$</li>\n",
    "            <li><b>ReLU activation function</b> (ReLU stands for Rectified Linear Unit): $$g(z) = max(0, z)$$</li>\n",
    "            <li><b>tanh activation function</b> (tanh is the hyperbolic tangent): $$g(z) = \\tanh(z)$$\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Apart from the linear activation function, these activation functions are <b>non-linear</b>, which\n",
    "        is important to the power of neural networks.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dot product</h2>\n",
    "<ul>\n",
    "    <li>Although artificial neurons are inspired by real neurons, really all we're doing is the dot \n",
    "        product of two vectors, followed by element-wise application of the activation function.\n",
    "    </li>\n",
    "</ul>\n",
    "<img src=\"images/nn_maths1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Relationship with Linear Models</h2>\n",
    "<ul>\n",
    "    <li>It should be clear that a single artificial neuron that uses the linear\n",
    "        activation function (which, in effect, does nothing) gives us the same linear models that\n",
    "        we had in Linear Regression.\n",
    "        <ul>\n",
    "            <li>The only differences are in terminology. In Linear Regression, the parameters are\n",
    "                referred to as coefficients (denoted $\\v{\\beta}$); in a neuron, the parameters\n",
    "                are the weights (denoted $\\v{w}$) and the bias ($b$).\n",
    "            </li>\n",
    "            <li>If we learn the values of the weights and bias using MSE as our loss function, then we will be doing\n",
    "                OLS regression.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It should be clear that a single artificial neuron that uses the sigmoid \n",
    "        activation function gives us the same models that we had when using Logistic Regression\n",
    "        for binary classification. (And we could learn the weights using the binary cross-entropy function as our\n",
    "        loss function.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Layers of Neurons</h1>\n",
    "<ul>\n",
    "    <li>We don't usually have just one neuron. We have a <b>layer</b>, containing several neurons.</li>\n",
    "    <li>For now let's consider what is called a <b>dense layer</b> (also a <b>fully-connected layer</b>):\n",
    "        <ul>\n",
    "            <li>every input is connected to every neuron in the layer.\n",
    "                <figure>\n",
    "                    <img src=\"images/layer.png\" />\n",
    "                </figure>\n",
    "            </li>\n",
    "       </ul>\n",
    "   </li>\n",
    "   <li>So now we have more than one output, one per neuron. But each is calculated in the same way as before:\n",
    "       compute a weighted sum of the inputs; apply the activation function to the weighted sum.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Matrix multiplication</h2>\n",
    "<ul>\n",
    "    <li>Suppose there are $p$ neurons in this layer. We can put all the weights into a $m \\times p$ matrix:\n",
    "        <ul>\n",
    "            <li>The first column contains the weights on the inputs going into the first neuron in the layer.\n",
    "                There will be $m$ weights.\n",
    "            </li>\n",
    "            <li>The second column contains the weights going into the second neuron.</li>\n",
    "            <li>And so on, one column for each of the $p$ neurons in the layer.</li>\n",
    "        </ul>\n",
    "        Call this matrix of weights $\\v{W}$.\n",
    "    </li>\n",
    "    <li>We can put each neuron's bias into a (row) vector, $\\v{b}$. This vector has $p$ values in it.</li>\n",
    "    <li>If we assume $\\v{x}$ is a row vector (as before), we can obtains all the outputs with simple calculations:\n",
    "        $$\\v{z} = \\v{b} + \\v{x}\\v{W}$$\n",
    "        &mdash; a matrix multiplication to get all the weighted sums to which we add the biases, giving a vector\n",
    "        $\\v{z}$ with $p$ cells in it. Then we apply $g$ element-wise to $\\v{z}$:\n",
    "        $$\\v{a} = g(\\v{z})$$\n",
    "        $\\v{a}$ is a vector with $p$ elements in it, one output per neuron in this layer.\n",
    "    </li>\n",
    "</ul>\n",
    "<img src=\"images/nn_maths2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multi-Layer Neural Networks</h1>\n",
    "<ul>\n",
    "    <li>We don't usually have just one layer. We have multiple layers.</li>\n",
    "    <li>Let's assume they are also <b>dense layers</b>, e.g.:\n",
    "        <figure>\n",
    "            <img src=\"images/layers.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>These neural networks contain:\n",
    "        <ul>\n",
    "            <li>an <b>input layer</b> (although this is not a layer of neurons);</li>\n",
    "            <li>one or more <b>hidden layers</b>;</li>\n",
    "            <li>an <b>output layer</b>.</li>\n",
    "        </ul>\n",
    "        Every neuron has a bias.\n",
    "    </li>\n",
    "    <li>The <b>depth</b> of a multi-layer neural network is simply the number of <em>layers of neurons</em>.\n",
    "        <ul>\n",
    "            <li>What is the depth of the network in the diagram above?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We would say that the network shown in the diagram is a <b>layered</b>, <b>dense</b>, <b>feedforward</b>\n",
    "        network.\n",
    "        <ul>\n",
    "            <li>These networks have the simplest <b>architecture</b> (structure).</li>\n",
    "            <li>In later lectures, we will see networks: \n",
    "                <ul>\n",
    "                    <li>where there may be 'splits' and 'merges';</li>\n",
    "                    <li>where not every layer is densely-connected; and</li>\n",
    "                    <li>where outputs may feedback to the earlier layers (<b>recurrent networks)</b>.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Matrix multiplication again</h2>\n",
    "<ul>\n",
    "    <li>Suppose there are $p^{(0)}$ neurons in the first layer, layer 0. We can put all the weights into a \n",
    "    $m \\times p^{(0)}$ matrix. Call this matrix of weights $\\v{W}^{(0)}$. We can put the biases of the\n",
    "        neurons in layer 0 into a (row) vector with $p$ elements; call this vector $\\v{b}^{(0)}$.\n",
    "    </li>\n",
    "    <li>Suppose there are $p^{(1)}$ neurons in the second layer. We can put all the weights between the first\n",
    "        layer and the second layer into a $p^{(0)} \\times p^{(1)}$ matrix, called $\\v{W}^{(1)}$.\n",
    "        <ul>\n",
    "            <li>Why are its dimenions $p^{(0)} \\times p^{(1)}$?</li>\n",
    "        </ul>\n",
    "        And we can put all the biases into a (row) vector with $p^{(1)}$ elements, $\\v{b}^{(1)}$.\n",
    "    </li>\n",
    "    <li>If we assume $\\v{x}$ is a row vector (as before), we can obtain all the outputs of the first layer \n",
    "        with simple calculations:\n",
    "        $$\\v{z}^{(0)} = \\v{b}^{(0)} + \\v{x}\\v{W}^{(0)}$$\n",
    "        $$\\v{a}^{(0)} = g(\\v{z}^{(0)})$$\n",
    "    </li>\n",
    "    <li>Then we can obtain all the outputs of the second layer with similar calculations:\n",
    "        $$\\v{z}^{(1)} = \\v{b}^{(1)} + \\v{a}^{(0)}\\v{W}^{(1)}$$\n",
    "        $$\\v{a}^{(1)} = g(\\v{z}^{(1)})$$\n",
    "    </li>\n",
    "    <li>If there are more layers, then we just do more matrix multiplications followed by element-wise application\n",
    "        of $g$.\n",
    "    </li>\n",
    "</ul>\n",
    "<img src=\"images/nn_maths3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In fact, as you've seen before, when we make predictions for unseen examples (inference), \n",
    "        we often want predictions, not for a single\n",
    "        object $\\v{x}$, but for a set of objects $\\v{X}$. This is also true during training, in the case of Batch\n",
    "        Gradient Descent and Mini-Batch Gradient Descent.\n",
    "    </li>\n",
    "    <li>This is a simple generalization of the above:\n",
    "        $$\\v{Z}^{(0)} = \\v{b}^{(0)} + \\v{X}\\v{W}^{(0)} \\mbox{ and } \\v{A}^{(0)} = g(\\v{Z}^{(0)})$$\n",
    "        $$\\v{Z}^{(1)} = \\v{b}^{(1)} + \\v{A}^{(0)}\\v{W}^{(1)} \\mbox{ and } \\v{A}^{(1)} = g(\\v{Z}^{(1)})$$\n",
    "        &hellip;and so on.\n",
    "    </li>\n",
    "</ul>\n",
    "<img src=\"images/nn_maths4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This is all that a neural network consists of! They are just collections of:\n",
    "        <ul>\n",
    "            <li>matrix multiplications; and</li>\n",
    "            <li>element-wise activation functions.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Actually, that's not true. We need to generalize a little bit. They are just collections of:\n",
    "        <ul>\n",
    "            <li><b>affine transformations</b> (matrix multplication being one example of an affine\n",
    "                transformation, which are linear operations); and\n",
    "            </li>\n",
    "            <li>element-wise functions (activation functions being one example).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Looking at neural networks in this way also helps us realise that a neural network simply defines\n",
    "        a function as a composite of other functions. In the example above, the whole network computes the\n",
    "        following:\n",
    "        $$g^{(1)}( g^{(0)}( \\v{X}\\v{W}^{(0)} + \\v{b}^{(0)} )\\v{W}^{(1)} + \\v{b}^{(1)} )$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why do we want more layers?</h2>\n",
    "<ul>\n",
    "    <li>A single neuron (or layer of neurons) gives us linear models.</li>\n",
    "    <li>With linear models, there are problems we cannot solve.\n",
    "        <ul>\n",
    "            <li>For example, we cannot build a classifier that correctly classifies exclusive-or:\n",
    "                <img src=\"images/xor.png\" />\n",
    "            </li>\n",
    "            <li>Why not?</li>\n",
    "            <li>(A recent paper in <i>Science Magazine</i> claims that a single layer of biological neurons\n",
    "                <em>can</em> compute exclusive-or. If true, this confirms what we said earlier: artificial neural\n",
    "                networks are inspired by the human brain, but they are not a model of the human brain.)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But, with multiple layers of neurons and the non-linearities of their activation functions, we eliminate\n",
    "        these limitations.\n",
    "        <ul>\n",
    "            <li>E.g. if you search online, you'll find examples of two-layer networks that can correctly classify\n",
    "                exclusive-or.\n",
    "            </li>\n",
    "            <li>Other things being equal, each extra hidden layer enlarges the set of hypotheses that the \n",
    "                network can represent: increasing complexity.\n",
    "            </li>\n",
    "            <li>In fact, the <b>universal approximation theorem</b> states that a feed-forward network with a\n",
    "                finite\n",
    "                (but abitrarily large) single hidden layer can approximate any\n",
    "                continuous function (to any desired precision), under mild assumptions on the activation function.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Training a Neural Network</h1>\n",
    "<ul>\n",
    "    <li>Brains learn by strengthening or weakening the connections between biological neurons. Analogously,\n",
    "        neural networks learn by modifying the values of the weights and biases.\n",
    "    </li>\n",
    "    <li>It is our job to decide on the neural network architecture.</li>\n",
    "    <li>And it is our job to choose the values of numerous <b>hyperparameters</b> that we will encounter.</li>\n",
    "    <li>But we use a dataset and a learning algorithm to find the values of the the network's <b>parameters</b>.\n",
    "        <ul>\n",
    "            <li>The parameters of a neural network are its weights and biases.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A lot of this is done using <b>supervised learning</b>:\n",
    "        <ul>\n",
    "            <li>So we need a <b>labeled dataset</b>;</li>\n",
    "            <li>a <b>loss function</b>; and</li>\n",
    "            <li>a learning algorithm known as <b>backpropagation</b> (or <b>backprop</b>) that uses\n",
    "                some variant of <b>Gradient Descent</b>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Deep Learning</h1>\n",
    "<ul>\n",
    "    <li>The word 'deep' in 'deep learning' does not mean profound.</li>\n",
    "    <li>In deep learning, we have 'lots' of layers &mdash; tens or even hundreds.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Drivers of Deep Learning</h2>\n",
    "<ul>\n",
    "    <li>Hardware:\n",
    "        <ul>\n",
    "            <li>Faster CPUs but then highly-parallel Graphical Processing Units (GPus) and now specially-designed\n",
    "                Tensor Processing Units (TPUs).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Data:\n",
    "        <ul>\n",
    "            <li>Sensors and the Internet have made vast datasets available: text, images, video, &hellip;\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Algorithmic advances:\n",
    "        <ul>\n",
    "            <li>The core ideas have been around a long time: Perceptrons (1950s), backpropagation\n",
    "                (1980s or earlier), convolutional networks (1980s), LSTMs (1990s), &hellip;\n",
    "            </li>\n",
    "            <li>But new ideas from 2010 onwards:\n",
    "                better weight initialization, batch normalization, different activation functions, variants of SGD, \n",
    "                numerous ways to avoid overfitting, new architectures, &hellip;\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Freeware:\n",
    "        <ul>\n",
    "            <li>Toolkits/APIs;</li>\n",
    "            <li>Educational resources.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Money!\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Applications of Deep Learning</h2>\n",
    "<ul>\n",
    "    <li>It is excelling at 'perceptual' tasks, e.g.\n",
    "         <ul>\n",
    "             <li>image classification;</li>\n",
    "             <li>image segmentation;</li>\n",
    "             <li>speech recognition;</li>\n",
    "             <li>handwriting transcription.</li>\n",
    "         </ul>\n",
    "    </li>\n",
    "    <li>But it is finding ever wider application:\n",
    "        <ul>\n",
    "             <li>video recommendation;</li>\n",
    "             <li>machine translaton;</li>\n",
    "             <li>text-to-speech;</li>\n",
    "             <li>question-answering;</li>\n",
    "             <li>autonomous driving;</li>\n",
    "            <li>the protein folding problem (<a href=\"https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">AlphaFold</a>);</li>\n",
    "             <li>super-human game playing (e.g. AlphaGo).</li>\n",
    "         </ul>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Appendix</h1>\n",
    "<h2>Representations</h2>\n",
    "<ul>\n",
    "    <li>One way of thinking about Machine Learning:\n",
    "        <ul>\n",
    "            <li>It uses guidance from a feedback signal to automatically find transformations that turn \n",
    "                input data into more useful representations.\n",
    "            </li>\n",
    "            <li>E.g. in the case of supervised learning, the feedback comes from the loss function and the\n",
    "                algorithm seeks a representation that is closer to the target outputs.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Deep learning is about jointly finding <em>successive layers of representations</em>, usually\n",
    "        in the form of the layers of a neural network.\n",
    "        <ul>\n",
    "            <li>The network takes in vectors (examples).</li>\n",
    "            <li>The first layer in some sense transforms the input vectors into new vectors &mdash; a different representation of the input examples.\n",
    "            </li>\n",
    "            <li>The second layer transforms again into new vectors &mdash; another representation.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Since each layer produces a new representations, one way of thinking about this is, for the \n",
    "        kinds of tasks on which it is successful, deep learning <em>automates feature engineering</em>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
