{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Training a Neural Network</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp\n",
    "\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "# Load MNIST into four Numpy arrays\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = load_data()\n",
    "mnist_x_train = mnist_x_train.reshape((60000, 28 * 28))\n",
    "mnist_x_test = mnist_x_test.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<h1>Acknowledgements</h1>\n",
    "<ul>\n",
    "    <li>The <code>LearningRateFinder</code> class comes from Adrian Rosebrock's excellent\n",
    "        <a href=\"https://www.pyimagesearch.com/\">pyimagesearch</a> site. He credits the\n",
    "        <a href=\"http://github.com/amaiya/ktrain\">ktrain library</a> for the original code.\n",
    "    </li>\n",
    "    <li>The <code>CyclicLR</code> class comes from Brad Kenstler and others:\n",
    "        <a href=\"https://github.com/bckenstler/CLR/blob/master/clr_callback.py\">https://github.com/bckenstler/CLR/blob/master/clr_callback.py</a>.\n",
    "    </li>\n",
    "    <li>They both credit the following paper for both ideas:<br />\n",
    "        L. N. Smith, \"Cyclical Learning Rates for Training Neural Networks,\" \n",
    "        2017 IEEE Winter Conference on Applications of Computer Vision (WACV), \n",
    "        Santa Rosa, CA, 2017, pp. 464-472, doi: 10.1109/WACV.2017.58.\n",
    "    </li>\n",
    "</ul>\n",
    "-->\n",
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>We'll give an overview of Gradient Descent for neural networks.\n",
    "        <ul>\n",
    "            <li>Our goal is to get the 'flavour' of the algorithm.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then, we will discuss the Vanishing Gradients Problem and its solutions.</li>\n",
    "    <li>Finally, we'll discuss how to choose a learning rate.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient Descent for Neural Networks</h1>\n",
    "<ul>\n",
    "    <li>Let's start by reminding ourselves of (Batch) Gradient Descent for OLS regression:\n",
    "        <ul style=\"background: lightgrey; list-style: none\">\n",
    "            <li>initialize $\\v{\\beta}$ randomly\n",
    "            <li>\n",
    "                repeat until convergence\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        $\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$\n",
    "                    </li>\n",
    "                </ul>\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We see it making predictions $\\hat{\\v{y}} = \\v{X}\\v{\\beta}$ for all the examples $\\v{X}$ and \n",
    "        comparing them with target values, $\\v{y}$.\n",
    "    </li>\n",
    "    <li>And we see it updating all the parameters $\\v{\\beta}$ by an amount equal to the negative of\n",
    "        the gradients, multiplied by the learning rate $\\alpha$.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Backpropagation</h1>\n",
    "<ul>\n",
    "    <li>For neural networks, however, there is a problem.\n",
    "        <ul>\n",
    "            <li>At the output layer, we can straightforwardly compute loss: we can get the network's output\n",
    "                (its prediction) and we have the target value, because the training set is a labeled dataset.\n",
    "            </li>\n",
    "            <li>But we cannot straightforwardly compute loss at the hidden layers: we know what outputs their\n",
    "                neurons produce but we do not know what they should produce (target values). The labeled\n",
    "                dataset doesn't tell us the target values for hidden layer neurons.\n",
    "            </li>\n",
    "        </ul>  \n",
    "    </li>\n",
    "    <li>The solution is to assume that each neuron in layer $l$ is responsible for some part of the loss\n",
    "        of the neurons it is connected to in layer $l+1$. We'll refer to this amount as the 'error signal'.\n",
    "    </li>\n",
    "    <li>This leads to the idea of an algorithm that makes two passes through the network:\n",
    "        <ul>\n",
    "            <li>a <b>forward pass</b> to make predictions: we feed $\\v{X}$ in to the network and then work forwards\n",
    "                through the network, computing \n",
    "                activations layer by layer\n",
    "                until we reach the output layer;\n",
    "            </li>\n",
    "            <li>a <b>backward pass</b> to compute the gradients: we calculate loss at the output layer and then\n",
    "                work backwards through the network computing error signals layer by layer until we reach the\n",
    "                input layer.\n",
    "            </li>\n",
    "        </ul>\n",
    "        With these two steps completed, we have all the gradients, so we can update all the weights and biases.\n",
    "    </li>\n",
    "    <li>This very informal description helps you see why the calculation of the gradients (and sometimes\n",
    "        the entire algorithm) is referred to as <b>backpropagation</b> (or just <b>backprop</b>).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Demo</h2>\n",
    "<ul>\n",
    "    <li>We'll take a look at <a href=\"http://experiments.mostafa.io/public/ffbpann/\">http://experiments.mostafa.io/public/ffbpann/</a>\n",
    "    </li>\n",
    "    <li>Or we'l take a look at <a href=\"https://mlu-explain.github.io/neural-networks/\">https://mlu-explain.github.io/neural-networks/</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Backpropagation Algorithm</h2>\n",
    "<ul>\n",
    "    <li><b>Random initialization</b>: initialize all weights and biases randomly</li>\n",
    "    <li><b>Forward propagation</b>: make predictions for all the training examples:\n",
    "        <ul>\n",
    "            <li>Layer by layer from from layer 1 to layer $L$:\n",
    "                <ul>\n",
    "                    <li>Calculate the inputs to the units in that layer (weighted sums plus biases)</li>\n",
    "                    <li>Calculate the outputs of the units in that layer (using activation function)</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Backpropagation</b>:\n",
    "        <ul>\n",
    "            <li>Calculate the error signals $\\Delta$ at layer $L$</li>\n",
    "            <li>Layer by layer in reverse from layer $L-1$ to layer 1:\n",
    "                <ul>\n",
    "                    <li>Calculate the error signals $\\Delta$ for the units in that layer</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Update all the weights</b>: \n",
    "        $w^{(l)}_{i,j} \\gets w^{(l)}_{i,j} - \\alpha \\times a_i \\times \\Delta^{(l)}_j$\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Class exercise</h2>\n",
    "<ul>\n",
    "    <li>Back prop starts by initializing weights randomly.\n",
    "        <ul>\n",
    "            <li>E.g it was common to use a normal distribution with mean of 0 and standard deviation \n",
    "                of, e.g., 0.05.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A novice proposes instead to initialize them all to zero.</li>\n",
    "    <li>Why in general does this not make sense?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Backpropagation Algorithm (Vectorized)</h2>\n",
    "<ul>\n",
    "    <li><b>Random initialization</b>: initialize all weights randomly</li>\n",
    "    <li><b>Forward propagation</b>:\n",
    "        <ul>\n",
    "            <li>Calculate and store $\\v{Z}^{(1)} = \\v{X}\\v{W}^{(1)}$\n",
    "            <li>Layer by layer from layer $l=1$ to layer $L$:\n",
    "                <ul>\n",
    "                    <li>Calculate and store $\\v{A}^{(l)} = g^{(l)}(\\v{Z}^{(l)})$</li>\n",
    "                    <li>Calculate $\\v{Z}^{(l+1)} = \\v{A}^{(l)}\\v{W}^{(l+1)}$</li>\n",
    "                    <li>Calculate and store $\\v{G}^{(l)} = g'(\\v{Z}^{(l)})^T$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Backpropagation</b>:\n",
    "        <ul>\n",
    "            <li>Calculate $\\v{D}^{(L)} = (\\v{A}^{(L)} âˆ’ \\v{Y})^T$</li>\n",
    "            <li>Layer by layer in reverse from layer $l=L-1$ to layer 1:\n",
    "                <ul>\n",
    "                    <li>Calculate and store $\\v{D}^{(l)} = \\v{G}^{(l)} * \\v{W}^{(l)}\\v{D}^{(l+1)}$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Update all the weights</b>:\n",
    "        <ul>\n",
    "            <li>$\\v{W}^{(1)} = \\v{W}^{(1)} - \\alpha (\\v{D}^{(1)}\\v{X})^T$</li>\n",
    "            <li>$\\v{W}^{(l)} = \\v{W}^{(l)} - \\alpha (\\v{D}^{(l)}\\v{A}^{(l-1)})^T$ for all other values of $l$</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>For simplicity (!), I've omitted the biases.</li>\n",
    "    <li>We can see in this more precise version that some of the things that are calculated on the\n",
    "        forward pass get stored.\n",
    "    </li>\n",
    "    <li>These things can then be used on the backward pass.</li>\n",
    "    <li>Similarly, some of the things that are calculated on the backward pass get stored.</li>\n",
    "    <li>These things can then be used to update the weights.</li>\n",
    "    <li>This makes backprop much more efficient than it otherwise would be.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Autodiff</h2>\n",
    "<ul>\n",
    "    <li>Whoa! We have shown the update rules for a <em>particular</em> network and a <em>particular</em> loss function.</li>\n",
    "    <li>We would get different update rules if we changed:\n",
    "        <ul>\n",
    "            <li>the network, e.g. layers other than dense layers (batch normalization layers, convolutional\n",
    "                layers, etc.); and/or\n",
    "            </li>\n",
    "            <li>the loss function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Happily, we don't have to manually find the partial derivatives all over again.\n",
    "        <ul>\n",
    "            <li>Neural networks consist of layers of operations, each with simple, known derivatives.</li>\n",
    "            <li>Given that the network simply defines a composite function (see previous lecture), the \n",
    "                derivatives for the whole network can be obtained automatically by repeated use of the\n",
    "                <b>chain rule</b>:\n",
    "                <ul>\n",
    "                    <li>To differentiate a function of a function, $y = f(g(x))$, let $u = g(x)$ so that \n",
    "                        $y = f(u)$, then\n",
    "                        $$\\frac{dy}{dx} = \\frac{dy}{du} \\times \\frac{du}{dx}$$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Hence, modern frameworks such as TensorFlow can compute gradients automatically in the\n",
    "                backpropagation step.\n",
    "            </li>\n",
    "            <li>This is known as <b>autodiff</b> (or, for the way it is used by backprop, <b>reverse mode autodiff</b>).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<h1>The Vanishing Gradient Problem</h1>\n",
    "<ul>\n",
    "    <li>Each weight is updated by an amount proportional to the gradient of the loss function with respect to\n",
    "        that weight.\n",
    "    </li>\n",
    "    <li>But if the gradient is very small, the weight doesn't change much.\n",
    "        <ul>\n",
    "            <li>This may prevent the network from converging to a good approximation of\n",
    "                the target function.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We can now see that this is worse for deeper networks.\n",
    "        <ul>\n",
    "            <li>The error signal becomes ever smaller as it is backpropagated.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We look at three solutions:\n",
    "        <ul>\n",
    "            <li>Non-saturating activation functions;</li>\n",
    "            <li>Better initialization;</li>\n",
    "            <li>Batch normalization.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Non-Saturating Activation Functions</h2>\n",
    "<ul>\n",
    "    <li>Certain activation functions, including the sigmoid function, are one cause of\n",
    "        the vanishing gradient problem.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigma(z):\n",
    "    return 1 / (1 + exp(-z))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Sigmoid function\")\n",
    "xvals = np.linspace(-10, 10, 21)\n",
    "plt.plot(xvals, [sigma(xval) for xval in xvals], color = \"red\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylabel(r\"$\\sigma(z)$\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>When the input to this function becomes large (positive or negative), the function \n",
    "        <b>saturates</b> (i.e. becomes very flat).\n",
    "        <ul>\n",
    "            <li>When it saturates, its derivative is extremely close to 0 so there's not much gradient \n",
    "                to propagate back to earlier layers (and what little gradient there is\n",
    "                gets diluted as it propagates back).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Even when the gradient is at its greatest (when input $z$ is 0 and $\\sigma(z) = 0.5$), it is only 0.25.\n",
    "        <ul>\n",
    "            <li>So in the back propagation, gradients always diminish by a quarter or more.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This is why we rarely use the sigmoid function as the activation function in the hidden\n",
    "        layers of deep networks.\n",
    "    </li>\n",
    "    <li>Lots of alternatives have been proposed, including the <b>rectified linear unit</b>\n",
    "        activation function, ReLU\n",
    "        $$\\mbox{ReLU}(z) = \\max(0, z)$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(z):\n",
    "    return max(z, 0)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"ReLU activation function\")\n",
    "xvals = np.linspace(-10, 10, 21)\n",
    "plt.plot(xvals, [relu(xval) for xval in xvals], color = \"red\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylabel(\"$relu(z)$\")\n",
    "plt.ylim(0.0, 10.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Neurons that use the ReLU activation function have obvious problems too: \n",
    "        <ul>\n",
    "            <li>If their input (weighted sum) is negative, \n",
    "                <ul>\n",
    "                    <li>the output is zero; and</li>\n",
    "                    <li>the gradient is zero;</li>\n",
    "                </ul>\n",
    "                &mdash; and if this is true for all examples in the training set then, in effect, the neuron dies.\n",
    "            </li>\n",
    "            <li>The gradient changes abruptly at $z=0$, which can make Gradient Descent bounce around.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Alternatives to ReLU such as Leaky ReLU, ELU (Exponential Linear Unit) and Scaled ELU have been \n",
    "        proposed, having  at least some non-zero\n",
    "        gradient for negative inputs but they are slower to compute and they introduce further hyperparameters.\n",
    "    </li>\n",
    "    <li>We'll stick with ReLU in this module. Despite its problems, it remains a popular choice.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Better Random Initialization</h2>\n",
    "<ul>\n",
    "    <li>It turns out that vanishing gradients are more likely for certain ways of initializing the weights.</li>\n",
    "    <li>Perhaps the most typical method was to use a normal distribution with mean of 0 and standard deviation \n",
    "        of, e.g., 0.05.\n",
    "    </li>\n",
    "    <li>Better methods have been proposed.\n",
    "        <ul>\n",
    "            <li>One (of many) is Glorot uniform initialization (also called Xavier uniform initialization).</li>\n",
    "            <li>Happily, this is the Keras default.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch Normalization</h2>\n",
    "<ul>\n",
    "    <li>We previously studied the usefulness of feature scaling when doing Gradient Descent for,\n",
    "        e.g., linear regression.\n",
    "        <figure>\n",
    "            <img src=\"images/unscaled.png\" style=\"display: inline\" /> \n",
    "            <img src=\"images/scaled.png\" style=\"display: inline\" />\n",
    "        </figure>\n",
    "        &hellip; and we've been doing this to the features in our neural networks too.\n",
    "    </li>\n",
    "    <li>But, if this is a good idea for the inputs to the first hidden layer, why not use the same\n",
    "        idea for the inputs to subsequent layers?\n",
    "        <ul>\n",
    "            <li>In other words, we normalize the activations (outputs) of layer $l$ prior to them being used \n",
    "                as inputs to layer $l+1$.\n",
    "            </li>\n",
    "            <li>This will control the distribution of the values throughout the training process.</li>\n",
    "        </ul>\n",
    "     </li>\n",
    "     <li>This, in essence, is the idea of <b>batch normalization</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Other benefits of Batch Normalization</h3>\n",
    "<ul>\n",
    "    <li>Batch Normalization reduces the vanishing gradients problem so much, we can even use saturating\n",
    "        activation functions.\n",
    "    </li>\n",
    "    <li>Training becomes less sensitive to the method used for randomly initializing weights.</li>\n",
    "    <li>Much larger learning rates work (faster convergence) with less risk of divergence.</li>\n",
    "    <li>It acts like a regularizer.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Batch normalization in Keras</h3>\n",
    "<ul>\n",
    "    <li>Just add another layer!</li>\n",
    "    <li>E.g.\n",
    "        <pre>\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "        </pre>\n",
    "        This is how we will do batch normalization in CS4618.\n",
    "    </li>\n",
    "    <li>Ignore: in fact, there is some debate about whether we should batch \n",
    "         normalize the activations of\n",
    "        the layer (as above) or the weighted sum, before applying the activation function (as below):\n",
    "        <pre>\n",
    "x = Dense(512, activation=\"linear\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "        </pre>\n",
    "        We'll stick with the former, which is more concise.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploring Vanishing Gradients with MNIST</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mnist_network(activation, initializer, use_batch_norm):\n",
    "    inputs = Input(shape=(28 * 28,))\n",
    "    x = Rescaling(scale=1./255)(inputs)\n",
    "    x = Dense(512, activation=activation, kernel_initializer=initializer)(x)\n",
    "    if use_batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    outputs = Dense(10, activation=\"softmax\", kernel_initializer=initializer)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9401000142097473\n",
      "0.9552000164985657\n",
      "0.9401000142097473\n",
      "0.9491999745368958\n",
      "0.9757999777793884\n",
      "0.9810000061988831\n",
      "0.9742000102996826\n",
      "0.9800999760627747\n"
     ]
    }
   ],
   "source": [
    "networks = [\n",
    "    build_mnist_network(\"sigmoid\", \"random_normal\", False),\n",
    "    build_mnist_network(\"sigmoid\", \"random_normal\", True),\n",
    "    build_mnist_network(\"sigmoid\", \"glorot_uniform\", False),\n",
    "    build_mnist_network(\"sigmoid\", \"glorot_uniform\", True),\n",
    "    build_mnist_network(\"relu\", \"random_normal\", False),\n",
    "    build_mnist_network(\"relu\", \"random_normal\", True),\n",
    "    build_mnist_network(\"relu\", \"glorot_uniform\", False),\n",
    "    build_mnist_network(\"relu\", \"glorot_uniform\", True)\n",
    "]\n",
    "\n",
    "for network in networks:\n",
    "    network.fit(mnist_x_train, mnist_y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    test_loss, test_acc = network.evaluate(mnist_x_test, mnist_y_test, verbose=0)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We shouldn't read too much into the results above.\n",
    "        The ideas in this lecture apply to <em>deep</em> networks.\n",
    "        This network is not deep.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter Tuning</h1>\n",
    "<ul>\n",
    "    <li>First an observation about validation sets:\n",
    "        <ul>\n",
    "            <li>When using neural networks, we typically have a large dataset.</li>\n",
    "            <li>Hence, we use <i>holdout</i> to split the dataset into train, validation and test sets.</li>\n",
    "            <li>If you have a smaller dataset, where you can afford to split off a test set, but you cannot afford to split off a validation set, then you might want to use $k$-fold cross-validation, as we did in our scikit-learn examples.</li>\n",
    "            <li>Keras does not have any in-built cross-validation functions. You'd have to write your own. Or copy the code from Chollet's book: see the Boston House Price example in <a href=\"https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter04_getting-started-with-neural-networks.ipynb\">this notebook</a>.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Second, an observation about hyperparameter tuning:\n",
    "        <ul>\n",
    "            <li>Hyperparameter tuning involves training lots of different models with different values for the hyperparamaters, and comparing their performance on the validation data.</li>\n",
    "            <li>This is often a problem with neural networks: training can be slow (due to large datasets and models that have many parameters) and there can be quite a lot of hyperparameters. Hence, training lots of different models for hyperparameter tuning is very time-consuming.</li>\n",
    "            <li>People often  don't bother! They just pick hyperparameter values out of their heads, or accept the Keras defaults.</li>\n",
    "            <li>If you can afford to be more systematic, there is a separate library called KerasTuner. It automates hyperparameter tuning, similar to what we saw in scikit-learn.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Appendix</h1>\n",
    "<h2>Some of the details of Batch Normalization</h2>\n",
    "<ul>\n",
    "    <li>(No need to learn these details &mdash; and there are some notation abuses anyway!)</li>\n",
    "    <li>In summary, for a given layer, it standardizes the outputs of the neurons (subtract the mean, divide by\n",
    "        the standard deviation), then it scales the result and adds an offset.\n",
    "    </li>\n",
    "    <li>It standardizes the output of a previous activation layer by subtracting the batch mean and dividing\n",
    "        by the batch standard deviation:\n",
    "        <ul>\n",
    "            <li>Let $B$ be the batch of examples of size $m_B$.</li>\n",
    "            <li>The mean of a batch of activation values $\\mu_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B} a^{(i)}$.</li>\n",
    "            <li>Their standard deviation $\\sigma^2_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B}(a^{(i)} - \\mu_B)^2$.</li>\n",
    "            <li>Standardize them: $a^{(i)}_{\\mbox{norm}} = \\frac{a^{(i)} - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}$,\n",
    "                where\n",
    "                $\\epsilon$ is a small value to avoid division-by-zero problems.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But, while we want the different units to have comparable activation values (achieved by\n",
    "        standardizing), we don't necessarily want them to have mean 0 and standard deviation 1.\n",
    "    </li>\n",
    "    <li>So, we multiply by a scaling factor for the layer and add an offset for the layer.\n",
    "        <ul>\n",
    "            <li>Scale and add offset: $\\tilde{a}^{(i)} = \\gamma a^{(i)}_{\\mbox{norm}} + \\beta$\n",
    "            <li>$\\gamma$ and $\\beta$ are parameters and so they are also learned by the Gradient Descent.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<!--\n",
    "<h2>Learning Rate</h2>\n",
    "<ul>\n",
    "    <li>Among many hyperparameters, perhaps the most important is the learning rate.\n",
    "        <ul>\n",
    "            <li>This can affect whether training converges or diverges.</li>\n",
    "            <li>By changining the learning rate using a <b>learning rate schedule</b> (as we do in\n",
    "                <b>simulated annealing</b>), we can improve convergence of SGD and \n",
    "                Mini-Batch GD, and maybe avoid getting stuck in some local minima.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Here we'll discuss two things: (a) one way to choose a learning rate and (b) one\n",
    "        learning schedule.\n",
    "    </li>\n",
    "    <li>The techniques we look at have been popularized by their use in the online\n",
    "        <a href=\"https://www.fast.ai/\">FastAI course</a>.\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>Plotting loss</h3>\n",
    "<ul>\n",
    "    <li>Two notes about the Keras <code>fit</code> method:\n",
    "        <ul>\n",
    "            <li>We can specify a validation dataset (<code>validation_data</code>) or request that\n",
    "                the dataset be split (<code>validation_split</code>).\n",
    "            </li>\n",
    "            <li>It returns a <code>History</code> object whose <code>history</code> attribute\n",
    "                records, for each epoch, the training losses and validation losses, and also the\n",
    "                values for any metrics requested in the <code>compile</code> method.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So we can choose a learning rate and plot losses. \n",
    "        <ul>\n",
    "            <li>If loss comes down too slowly, the learning rate may be too small.</li>\n",
    "            <li>If loss starts to go up, then the learning rate may be too large (divergence).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "def build_mnist_model():\n",
    "    inputs = Input(shape=(28 * 28,))\n",
    "    x = Rescaling(scale=1./255)(inputs)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    outputs = Dense(10, activation=\"softmax\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "mnist_model = build_mnist_model()\n",
    "\n",
    "history = mnist_model.fit(mnist_x_train, mnist_y_train, epochs=25, batch_size=32, validation_split=0.2, verbose=0)\n",
    "<ul>\n",
    "    <li>We can plot training error and validation error (loss) on the $y$-axis and epochs on the $x$-axis\n",
    "        to see our progress.\n",
    "    </li>\n",
    "    <li>We will call this kind of plot a <b>training curve</b>.\n",
    "        <ul>\n",
    "            <li>How do the $x$-axes differ in a <b>validation curve</b>, <b>learning curve</b>\n",
    "                and <b>training curve</b>?\n",
    "            </li>\n",
    "            <li>What is each used for?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    " </ul>\n",
    " pd.DataFrame(history.history).plot()\n",
    " <ul>\n",
    "    <li>(There is an explanaton for why validation loss can be initially lower than training loss: they are\n",
    "        half an epoch out-of-step.)\n",
    "    </li>\n",
    "    <li>Here, the learning rate we chose seems OK.</li>\n",
    "    <li>If it weren't, we could change it, retrain and re-plot.</li>\n",
    "    <li>But this is hit-and-miss. One alternative is grid search or randomized search, but these are \n",
    "        expensive.\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>Finding a learning rate</h3>\n",
    "<ul>\n",
    "    <li>A popular alternative to hit-and-miss and grid search/randomized search is:\n",
    "        <ul>\n",
    "            <li>Train for just a few epochs, e.g. 5.</li>\n",
    "            <li>Start with a very low learning rate (e.g. 1e-10), then a  higher one, \n",
    "                then a higher one, and so on up to some high value (e.g. 10).\n",
    "            </li>\n",
    "            <li>Change the learning rate after each <em>batch</em>.</li>\n",
    "            <li>Plot the loss ($y$-axis) against learning rate ($x$-axis).</li>\n",
    "            <li>You should see loss decreasing slowly, then dropping sharply, and then increasing\n",
    "                sharply.\n",
    "            </li>\n",
    "            <li>The best learning rate is just before loss starts to rise: rule-of-thumb is 10 times\n",
    "                smaller than the bottom.\n",
    "            </li>\n",
    "        </ul>\n",
    "        From now on, use this as your learning rate.\n",
    "    </li>\n",
    "</ul>   \n",
    "mnist_model = build_mnist_model()\n",
    "\n",
    "lr_finder = LearningRateFinder(mnist_model)\n",
    "\n",
    "lr_finder.find(trainData=(mnist_x_train, mnist_y_train), \n",
    "               startLR=1e-10, endLR=1e+1, \n",
    "               epochs=5, batchSize=32, verbose=0)\n",
    "\n",
    "lr_finder.plot_loss(skipBegin=10, skipEnd=1, title=\"\")\n",
    "<ul>\n",
    "    <li>This seems to be recommending 10e-2 or 10e-3 (it can vary from\n",
    "        run to run).\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>1cycle scheduling</h3>\n",
    "<ul>\n",
    "    <li>If an optimizer uses a learning rate schedule, then most likely the schedule reduces the\n",
    "        learning rate over time.\n",
    "    </li>\n",
    "    <li>But reducing the learning rate may nevertheless leave us stuck on plateaus or in local minima.</li>\n",
    "    <li>1cycle scheduling, popularized by FastAI, uses two learning rates:\n",
    "        <ul>\n",
    "            <li>minimum learning rate; and</li>\n",
    "            <li>maximum learning rate.</li>\n",
    "            Simply, we change the learning rate after every batch: it grows from minimum to maximum,\n",
    "            then from maximum to minimum, then from\n",
    "            minimum to maximum, and so on.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>What should you use as the minimum and maximum learning rates?\n",
    "        <ul>\n",
    "            <li>One suggestion is to use the plot from the learning rate finder above: just after it starts\n",
    "                falling, and just before it starts growing.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "mnist_network = build_mnist_model()\n",
    "\n",
    "batch_size = 32\n",
    "step_size = 6 * mnist_x_train.shape[0] // 32\n",
    "\n",
    "clr = CyclicLR(base_lr=10e-3, max_lr=10e-1, step_size=step_size)\n",
    "\n",
    "history = mnist_model.fit(mnist_x_train, mnist_y_train, epochs=25, batch_size=batch_size, validation_split=0.2, \n",
    "                          callbacks=[clr], verbose=0)\n",
    "<ul>\n",
    "    <li>The recommendation is that <code>step_size</code> is some multiple of (e.g. $6\\times$) the number of\n",
    "        batches per epoch.\n",
    "    </li>\n",
    "</ul>\n",
    "test_loss, test_acc = mnist_model.evaluate(mnist_x_test, mnist_y_test)\n",
    "test_acc\n",
    "pd.DataFrame(history.history).plot()\n",
    "<ul>\n",
    "    <li>We can even see how the learning rate changes:</li>\n",
    "</ul>\n",
    "plt.plot(clr.history[\"lr\"])\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
