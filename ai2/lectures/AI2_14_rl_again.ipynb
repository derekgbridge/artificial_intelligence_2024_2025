{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Reinforcement Learning, Again</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<h1>Reinforcement Learning</h1>\n",
    "<!-- https://twitter.com/xiaxiaoqiang/status/1731094134134276310 -->\n",
    "<ul>\n",
    "    <li>There are many algorithms for RL, but the one we studied previously was $Q$-Learning.\n",
    "    </li>\n",
    "    <li>The problem with algorithms such as $Q$-Learning is the table, which has an entry for every state\n",
    "        paired with every action.\n",
    "        <ul>\n",
    "            <li>This does not scale well to problems that have many states and/or many actions.</li>\n",
    "            <li>It fails to generalise: similar states probably have similar $Q$-values.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The state-of-the-art solution is to use a deep neural network to represent and learn the $Q$-values.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep $Q$-Learning</h1>\n",
    "<ul>\n",
    "    <li>In <b>Deep $Q$-Learning</b> we use a deep neural network to represent and learn the $Q$-values.</li>\n",
    "    <li>We're going to look at one way of doing this: a DQN (<b>Deep $Q$-Network</b>).</li>\n",
    "    <li>A DQN predicts $Q$-values (regression!).</li>\n",
    "    <li>One way of doing this:\n",
    "        <ul>\n",
    "            <li>Input layer takes in a state and an action.</li>\n",
    "            <li>Output layer has just one neuron, which outputs the $Q$-value.</li>\n",
    "            <li>To choose which action to take in a state, we must activate the network repeatedly: once\n",
    "                for each action.\n",
    "            </li>       \n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Another way of doing this:\n",
    "        <ul>\n",
    "            <li>Input layer takes in just a state.</li>\n",
    "            <li>Output layer has one neuron per action to output their $Q$-values.</li>\n",
    "            <li>To choose which action to take in a state, we activate the network just once and choose the \n",
    "                action for the output neuron with the largest activation.\n",
    "            </li>\n",
    "        </ul>\n",
    "        This is what DQNs do. It is regression but, given a state, you're predicting more than one target value (one per action). (This is sometimes known as multivariate regression.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DQN</h2>\n",
    "<ul>\n",
    "    <li>Define a neural network (the second kind that was discussed above) and initialize its weights randomly.</li>\n",
    "    <li>Pseudocode (discussed in subsequent slides, below):\n",
    "<figure style=\"border: 1px solid black; background-color: #D0D0D0\">\n",
    "    <figcaption style=\"border-bottom: 1px solid black\">\n",
    "        Deep-Q-Learning($\\epsilon$)\n",
    "    </figcaption>\n",
    "    <ul>\n",
    "        <li>Initialize replay memory: $D = [\\,]$</li>\n",
    "        <li>$\\v{s} = \\mathit{SENSE}()$;</li>\n",
    "        <li>do forever\n",
    "            <ul>\n",
    "                <li>$\\mathit{rand} = $ a randomly-generated number in $[0,1)$;</li>\n",
    "                <li>if $\\mathit{rand} < \\epsilon$\n",
    "                    <ul>\n",
    "                        <li>Choose action $a$ randomly;</li>\n",
    "                    </ul>\n",
    "                    else\n",
    "                    <ul>\n",
    "                        <li>$a = \\argmax_a Q(\\v{s}, a)$;</li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>$r = \\mathit{EXECUTE}(a)$;</li>\n",
    "                <li>$\\v{s}' = \\mathit{SENSE}()$;</li>\n",
    "                <li>Store $\\langle\\v{s}, a, r, \\v{s}'\\rangle$ in $D$</li>\n",
    "                <li>Randomly choose a mini-batch $\\v{X}$ of examples from $D$</li>\n",
    "                <li>For each $\\langle \\v{s}, a, r, \\v{s}'\\rangle \\in \\v{X}$, calculate the target value, i.e.\n",
    "                    $r + \\gamma\\max_{a'}Q(\\v{s}', a')$\n",
    "                </li>\n",
    "                <li>Train the network on mini-batch $\\v{X}$ (one iteration only) using loss function\n",
    "                    $\\frac{1}{2}\\sum_{\\langle \\v{s}, a, r, \\v{s}'\\rangle \\in \\v{X}}[r + \\gamma\\max_{a'}Q(\\v{s}', a') - Q(s, a)]^2$\n",
    "                <li>$\\v{s} = \\v{s}'$;</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</figure>\n",
    "    </li>\n",
    "    <li>Keep in mind that the $Q$-values above ($Q(\\v{s}, a)$ and $Q(\\v{s}', a')$) are predicted by\n",
    "        the neural network\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploration vs. Exploitation</h2>\n",
    "<ul>\n",
    "    <li>We discussed this previously.</li>\n",
    "    <li>The $\\epsilon$-greedy policy is a simple solution.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DQN's loss function</h2>\n",
    "<ul>\n",
    "    <li>In supervised learning, the target values are fixed before learning begins.</li>\n",
    "    <li>In DQNs, the targets change!</li>\n",
    "    <li>Consider a transition $\\langle \\v{s}, a, r, \\v{s}'\\rangle$:\n",
    "        <ul>\n",
    "            <li>Suppose the agent is in state $\\v{s}$.</li>\n",
    "            <li>It feeds $\\v{s}$ into the network; the output neurons produce $Q$-values for each action.</li>\n",
    "            <li>These $Q$-values are the <em>predictions</em>.</li>\n",
    "            <li>It chooses the action $a$ that has highest $Q$-value: $\\argmax_a Q(\\v{s}, a)$.</li>\n",
    "            <li>It executes $a$, obtaining reward $r$ and transitioning to state $\\v{s}'$.</li>\n",
    "            <li>It feeds $\\v{s}'$ into the network; the output neurons produce $Q$-values for each action.</li>\n",
    "            <li>For action $a$ (the one we chose), \n",
    "                $r + \\gamma\\max_{a'}Q(\\v{s}', a')$ is a better estimate and so this is the \n",
    "                <em>target</em>.\n",
    "            </li>\n",
    "            <li>For the other actions (the ones we did not choose), the target is the same as the\n",
    "                prediction (error is 0).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We can use mean-squared-error for the loss function:\n",
    "        <ul>\n",
    "            <li>The square of the difference between target and prediction.</li>\n",
    "            <li>In effect, the loss is:\n",
    "                $$\\frac{1}{2}[\\underbrace{r + \\gamma\\max\\nolimits_{a'}Q(\\v{s}', a')}_{\\mathit{target}} - \\underbrace{Q(s, a)}_{\\mathit{prediction}}]^2$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Experience replay</h2>\n",
    "<ul>\n",
    "    <li>The previous slide implies that, in each sense-plan-act cycle, we train on just one example,\n",
    "        <ul>\n",
    "            <li>i.e. for transition $\\langle \\v{s}, a, r, \\v{s}'\\rangle$, we described the target value\n",
    "                and prediction.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But this takes ages to converge.</li>\n",
    "    <li>Instead, DQNs use <b>experience replay</b>:\n",
    "        <ul>\n",
    "            <li>All the experiences $\\langle \\v{s}, a, r, \\v{s}'\\rangle$ are stored in a \n",
    "                so-called 'replay memory'.</li>\n",
    "            <li>When training, the DQN chooses a random mini-batch from the replay memory.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Further tweaks</h2>\n",
    "<ul>\n",
    "    <li>The presentation above simplifies the algorithm a little.</li>\n",
    "    <li>It ignores an extra if-statement for handling the situation where the loop terminates (e.g. when a\n",
    "        game is over).</li>\n",
    "    <li>More importantly, DQNs include lots of other 'tricks' to improve convergence, including:\n",
    "        <ul>\n",
    "            <li>$\\epsilon$ decreases, e.g. from 1 to 0.1 over time.</li>\n",
    "            <li>The use of two neural networks (one for predicting targets and one for everything else).\n",
    "                <ul>\n",
    "                    <li>Weights from the second one are copied into the first one periodically.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Error clipping, reward clipping, &hellip;</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We won't concern ourselves with these in CS4619!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DQNs for Atari Games</h1>\n",
    "<ul>\n",
    "    <li>The DQN algorithm was developed by a company called DeepMind (now owned by Google).</li>\n",
    "    <li>In 2013/2014, they trained DQNs to play Atari 2600 video games.</li>\n",
    "    <li>States of the game:\n",
    "        <ul>\n",
    "            <li>You might think that the state of the game is represented using a game-specific\n",
    "                data structure.\n",
    "            </li>\n",
    "            <li>The cool part: instead, the state of the game is an image (array of pixels) &mdash; the same\n",
    "                as you see on the screen.\n",
    "                <ul>\n",
    "                    <li>This is a generic way of representing the state of these games.</li>\n",
    "                    <li>It is what makes their DQN applicable to so many different games:\n",
    "                        all you need are images from the game after each user action.\n",
    "                    </li>\n",
    "                    <li>It does mean that the lower layers in the neural network need to be convolutional layers\n",
    "                        &mdash; to do some image processing.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    <li>Mnih et al.: <i>Playing Atari with Deep Reinforcement Learning</i> CoRR, abs/1312.5602, 2013\n",
    "        (<a href=\"https://arxiv.org/abs/1312.5602\">https://arxiv.org/abs/1312.5602</a>)\n",
    "        <ul>\n",
    "            <li>describes seven games, outperforming humans on three of them.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Mnih et al.: <i>Human-level control through deep reinforcement learning</i>,\n",
    "        Nature volume 518, pages 529–533, 2015\n",
    "        (<a href=\"https://www.nature.com/articles/nature14236\">https://www.nature.com/articles/nature14236</a>)\n",
    "        <ul>\n",
    "            <li>describes 49 games, outperforming humans on half of them.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>States as images</h2>\n",
    "<ul>\n",
    "    <li>If states are images then, in principle, we can find out the positions of all the objects\n",
    "        (e.g. the bricks and paddle in Breakout).\n",
    "    </li>\n",
    "    <li>But we cannot find out their speed or direction of travel.</li>\n",
    "    <li>So DeepMind chose to represent a single state by four images:\n",
    "        <ul>\n",
    "            <li>After an action, the new state is the current screen image and the preceding three.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>They also do a little preprocessing to reduce image sizes: \n",
    "        <ul>\n",
    "            <li>convert RGB to grayscale;</li>\n",
    "            <li>scale down from $210 \\times 160$ pixels to $84 \\times 84$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So, if $m$ is the mini-batch size, then the  input shape is $(m, 84, 84, 4)$. Why?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>OpenAI Gym</h2>\n",
    "<ul>\n",
    "    <li>We don't want to programme Atari games from scratch ourselves.</li>\n",
    "    <li><i>Gymnasium</i> (<a href=\"https://github.com/Farama-Foundation/Gymnasium\">https://github.com/Farama-Foundation/Gymnasium</a>, <a href=\"https://gymnasium.farama.org/\">Documentation</a>) provides a range of agents and environments for working on RL, e.g. Atari games, board games, etc.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-Agents</h2>\n",
    "<ul>\n",
    "    <li>TF-Agents (<a href=\"https://www.tensorflow.org/agents\">https://www.tensorflow.org/agents</a>) \n",
    "        is a tensorflow library that implement DQNs and other deep learning versions of RL.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Demo code</h2>\n",
    "<ul>\n",
    "    <li>Take a look at notebook 18 <a href=\"https://github.com/ageron/handson-ml2\">here</a>.</li>\n",
    "    <li>You can download it and run it yourself on your own machine (if you have a GPU) or on Google Colab.</li>\n",
    "    <li>Or if you click on it, you'll see it contains a button (\"Run in Google Colab\") that you can click on.</li>\n",
    "    <li>In Google Colab, chose Runtime &gt; Change runtime type &gt; and choose a GPU; Save. Then execute as many cells as \n",
    "        you want.\n",
    "        If you only want to execute the DQN for Breakout, then execute the first cell + the one that contains\n",
    "        the definition of the <code>plot_animation</code> function + the cells in the section called\n",
    "        <i>Using TF-Agents to Beat Breakout</i>.\n",
    "    </li>\n",
    "    <li>Here are animated GIFs of the agent that I trained with <code>max_length=100000</code> and \n",
    "        <code>n_iterations=10000-100000</code>:\n",
    "        <figure style=\"text-align: center;\">\n",
    "               <figcaption>After 40000 iterations:</figcaption>\n",
    "               <img src=\"images/breakout3.gif\" />\n",
    "        </figure>\n",
    "         <figure style=\"text-align: center;\">\n",
    "               <figcaption>After 100000 iterations:</figcaption>\n",
    "               <img src=\"images/breakout9.gif\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Concluding Remarks</h1>\n",
    "<ul>\n",
    "    <li>The lecture gives a flavour of using deep learning for RL.</li>\n",
    "    <li>There are many more complex variants (e.g. Dueling Deep $Q$-Networks and Double Deep $Q$-Networks) \n",
    "        including ones that lift the assumption of a finite set of actions (e.g. Actor-Critic models).\n",
    "    </li>\n",
    "    <li>Let's also mention AlphaGo, AlphaGo Zero and AlphaZero:\n",
    "        <ul>\n",
    "            <li>March 2016, AlphaGo beat 18 times world champion Lee Sedol 4-1.</li>\n",
    "            <li>October 2017, AlphaGo Zero (trained on self-play only) beat AlphaGo 100-0.\n",
    "                <figure>\n",
    "                    <img src=\"images/alphago.png\" />\n",
    "                </figure>\n",
    "            </li>\n",
    "            <li>December 2017, AlphaZero beats StockFish at chess and Elmo at Shogi (Japanse chess).</li>\n",
    "        </ul>\n",
    "        In principle, we can train AlphaZero to play any perfect information game from self-play only. However,\n",
    "        AlphaZero is trained separately &mdash;from scratch&mdash; on each game.\n",
    "    </li>\n",
    "    <li>Even more interestingly, DeepMind has been working on a system that can be trained to play many different\n",
    "        games. The games are things like Hide &amp; Seek, Tag, Catch-the-Flag and so on, played in a simulator \n",
    "        that allows the creation of environments consisting of players and blocks. The agent uses Deep RL, with a\n",
    "        reward function that is normalized to reward performance across the wide range of training tasks. It engages\n",
    "        in a process of continuous learning. The developers claim that the agent learns \"interesting emergent\n",
    "        heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, \n",
    "        and co-operation\": <a href=\"https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\">Blog post (with link to videos)</a>; <a href=\"https://arxiv.org/pdf/2107.12808.pdf\">Longer manuscript</a>.\n",
    "    </li>\n",
    "    <li>In the same vein, Google has shown that a single transformer-based reinforcement learning model, with a single set of weights, trained can play up to 46 Atari games simultaneously at close-to-human performance: <a href=\"https://blog.research.google/2022/07/training-generalist-agents-with-multi.html\">Blog post</a>; <a href=\"https://arxiv.org/abs/2205.15241\">Longer manuscript</a>.</li>\n",
    "    <li>With similar goals but a different approach, and starting from the observation that humans can learn quickly across many tasks, MIT has been working on equipping RL with knowledge of how humans learn and act &mdash; how they explore the world, how they model its causal structure, and how they use these models to plan actions that achieve their goals. They illustrate with a video game playing agent called EMPA (the\n",
    "Exploring, Modeling, and Planning Agent), tested on about 90 Atari-like games:\n",
    "        <a href=\"https://arxiv.org/pdf/2107.12544.pdf\">Manuscript</a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
