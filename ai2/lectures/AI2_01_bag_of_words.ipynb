{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Bag of Words</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow import convert_to_tensor, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  base_dir = \"./drive/My Drive/Colab Notebooks/\" # You may need to change this, depending on where your notebooks are on Google Drive\n",
    "else:\n",
    "  base_dir = \".\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Natural Language Processing</h1>\n",
    "<ul>\n",
    "    <li>Languages:\n",
    "        <ul>\n",
    "            <li><b>Natural languages</b> are languages, such as English, which arise through some sort of 'cultural evolution'.</li>\n",
    "            <li><b>Formal languages</b> are ones that are designed by humans, e.g. programming languages such as Python.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Formal languages have rules (syntax rules and perhaps a formal semantics). Natural languages, by contrast, follow certain cognitive principles. Linguists might attempt to formalize rules for a natural language, but users of the language are not constrained to follow the rules.</li>\n",
    "    <li>Natural Language Processing (NLP) is a phrase that covers the work we do in AI on Natural Language Understanding (NLU) and Natural Language Generation (NLG).</li>\n",
    "    <li>History:\n",
    "        <ul>\n",
    "            <li>Early work on NLP was quite ad-hoc.</li>\n",
    "            <li>Then, in the 1980s, we equipped our NLP systems with the kinds of rules written by linguists. The coverage of the systems improved enormously, but they were very brittle &mdash; they failed whenever they were faced with the kind of English that we use everyday.</li>\n",
    "            <li>From the late 1980s, from large datasets, we used Machine Learning (and, later, Deep Learning) to train systems to make predictions:\n",
    "                <ul>\n",
    "                    <li>E.g. spam classifiers, sentiment analysers, topic classifiers, next-word predictors (autocompletion), machine translation, text summarization, &hellip;</li>\n",
    "                    <li>These systems are useful. You use them all the time! Their performance may be impressive (and becoming ever more so).</li>\n",
    "                    <li>But, like all Machine Learning, they work by finding regularities in the training data. They are a long way from <i>understanding</i> language.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Free-Form Text</h1>\n",
    "<ul>\n",
    "    <li>We've looked at AI systems that can handle structured data (i.e. numeric-valued and non-numeric-valued features) and AI systems that can handle images.\n",
    "    </li>\n",
    "    <li>Suppose instead the objects in your dataset are <b>documents</b>, rather than houses, students, irises or photos.\n",
    "        <ul>\n",
    "            <li>E.g. web pages, tweets, blog posts, emails, posts to Internet forums and chatrooms, &hellip;</li>\n",
    "            <li>They might have a little structure to them (headings and so on), but they are primarily\n",
    "                <b>free-form text</b>, written in a natural language, such as English.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Many AI algorithms can only handle vectors of numbers. So one way to apply AI techniques to \n",
    "        a dataset of documents is to convert the raw text in the documents into vectors of numbers.\n",
    "    </li>\n",
    "    <li><strong>Note that in this lecture, each document becomes one vector.</strong> In the next lecture, each word becomes one vector and hence a document becomes a list of vectors.\n",
    "    </li>\n",
    "    <!--\n",
    "    <li>Our treatment of this will be brief and high-level, since many of you studied\n",
    "        <i>CS4611 Information Retrieval</i>, where this is covered in depth.\n",
    "    </li>\n",
    "    -->\n",
    "    <li><!-- Furthermore, we'll --> We'll illustrate this lecture mostly using scikit-learn \n",
    "        although its facilities for handling text are quite limited. \n",
    "        If you really want to do AI with text, consider a more powerful library such as <i>NLTK</i>\n",
    "        (<a href=\"http://www.nltk.org/\">http://www.nltk.org/</a>) or the <i>Stanford Natural Language\n",
    "        Processing Toolkit</i> \n",
    "        (<a href=\"https://nlp.stanford.edu/software/\">https://nlp.stanford.edu/software/</a>).\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sets</h1>\n",
    "<h2>Background Maths</h2>\n",
    "<ul>\n",
    "    <li>A <b>set</b> is a collection of objects with two properties:\n",
    "        <ul>\n",
    "            <li>Order is not important,<br />\n",
    "                e.g. $\\Set{a, c, f} = \\Set{c, f, a}$\n",
    "            </li>\n",
    "            <li>Duplicates are not allowed,<br />\n",
    "                e.g. $\\Set{a, c, a, f} = \\Set{a, c, f}$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The set of all possible objects $U$ is called the <b>universal set</b>, e.g. $U = \\Set{a, b, c, d, e, f, g}$.</li>\n",
    "</ul>\n",
    "<h2>Background Computer Science</h2>\n",
    "<ul>\n",
    "    <li>There are many data structures we can use to store sets, e.g. linked lists, binary search trees, &hellip;</li>\n",
    "    <li>But we can describe a set by a binary-valued vector.\n",
    "        <ul>\n",
    "            <li>E.g. if $U = \\Set{a, b, c, d, e, f, g}$, then we can represent the set $\\Set{a, c, f}$ as follows:\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <td>$a$</td><td>$b$</td><td>$c$</td><td>$d$</td><td>$e$</td><td>$f$</td><td>$g$</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>$1$</td><td>$0$</td><td>$1$</td><td>$0$</td><td>$0$</td><td>$1$</td><td>$0$</td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </li>\n",
    "        </ul>\n",
    "        Then, we can store the set using the same data structure that we use for storing vectors, e.g. numpy arrays. \n",
    "    </li>\n",
    "    <li>In fact, if $U$ is large but the sets we store tend to be much smaller, then our vectors will be\n",
    "        <b>sparse</b> (mostly zero).\n",
    "    </li>\n",
    "    <li>It may be more efficient to use a data structure that only stores the non-zero elements. numpy has several data structures for this (e.g. <code>csr_matrix</code>).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bags</h1>\n",
    "<h2>Background Maths</h2>\n",
    "<ul>\n",
    "    <li>A <b>bag</b> is a collection of objects with one property:\n",
    "        <ul>\n",
    "            <li>Order is not important,<br />\n",
    "                e.g. $\\Set{a, c, f} = \\Set{c, f, a}$\n",
    "            </li>\n",
    "            <li>This time, duplicates are allowed,<br />\n",
    "                e.g. $\\Set{a, c, f} \\neq \\Set{a, c, a, f}$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<h2>Background Computer Science</h2>\n",
    "<ul>\n",
    "    <li>We can describe a bag by a numeric-valued vector, where the numbers are the frequencies with which the elements occur.\n",
    "        <ul>\n",
    "            <li>E.g. if $U = \\Set{a, b, c, d, e, f, g}$, then we can represent the bag $\\Set{a, c, a,  f}$ as follows:\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <td>$a$</td><td>$b$</td><td>$c$</td><td>$d$</td><td>$e$</td><td>$f$</td><td>$g$</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>$2$</td><td>$0$</td><td>$1$</td><td>$0$</td><td>$0$</td><td>$1$</td><td>$0$</td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        We can store these as numpy arrays or, if they are sparse, we can use numpy's sparse data structures.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bag-of-Words</h1>\n",
    "<ul>\n",
    "    <li>We will represent each document by a bag-of-words.</li>\n",
    "    <li>This will lose lots of information. Start thinking about what we will lose!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Running example</h2>\n",
    "<p>\n",
    "    Suppose our dataset contains just these three documents:\n",
    "</p>\n",
    "<table style=\"border-collapse:collapse;\">\n",
    "    <tr>\n",
    "        <th style=\"border: 1px solid black;\">Tweet 0</th>\n",
    "        <th style=\"border: 1px solid black;\">Tweet 1</th>\n",
    "        <th style=\"border: 1px solid black;\">Tweet 2</th></tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black;\">\n",
    "            No one is born hating another person because of the color of his skin or his background \n",
    "            or his religion.\n",
    "        </td>\n",
    "        <td style=\"border: 1px solid black;\">\n",
    "            People must learn to hate, and if they can learn to hate, they can be taught to love.</td>\n",
    "        <td style=\"border: 1px solid black;\">\n",
    "            For love comes more naturally to the human heart than its opposite.</td>\n",
    "     </tr>\n",
    "     <caption style=\"caption-side: bottom; text-align: center\">\n",
    "         Three tweets from Barack Obama, quoting Nelson Mandela\n",
    "     </caption>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>\n",
    "<ul>\n",
    "    <li>First, we must <b>tokenize</b> each document. This means splitting it into <b>tokens</b>.</li> <!--<b>terms</b>.</li>-->\n",
    "    <li>In our simple treatment, the tokens <!--terms--> are just the words, ignoring punctuation and making everything\n",
    "        lowercase.\n",
    "        <ul>\n",
    "            <li>e.g. if we tokenize \"People must learn to hate, and if they can learn to hate, they can be taught to love.\", we get a sequence of 18 tokens: \"people must learn to hate and if they can learn to hate they can be taught to love\"</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "   <li>In reality, tokenization is surprisingly complicated,\n",
    "       <ul>\n",
    "           <li>e.g. should we keep the punctuation as separate tokens?</li>\n",
    "           <li>e.g. should we treat \"People\" and \"people\" as different tokens?</li>\n",
    "           <li>e.g. is \"don't\" one token <!--term--> or two or three?</li>\n",
    "           <li>e.g. maybe pairs of consecutive words (so-called 'bigrams') could also be treated as if they were single tokens <!--terms--> (\"no one\", \"one is\",\n",
    "               \"is born\")</li>\n",
    "           <li>and so on.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Stop-words</h2>\n",
    "<ul>\n",
    "    <li>Optionally, discard <b>stop-words</b>:\n",
    "    <li>Stop-words are common words such as \"a\", \"the\", \"in\", \"on\", \"is, \"are\",&hellip;</li>\n",
    "    <li>Sometimes discarding them helps, or does no harm, e.g. spam detection.</li>\n",
    "    <li>Other times, you lose too much, e.g. web search engines (\"To be, or not to be\").</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running example</h2>\n",
    "<ul>\n",
    "    <li>After tokenization and discarding stop-words:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"border: 1px solid black;\">Tweet 0</th>\n",
    "        <th style=\"border: 1px solid black;\">Tweet 1</th>\n",
    "        <th style=\"border: 1px solid black;\">Tweet 2</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black;\">\n",
    "            born hating person color skin background religion\n",
    "        </td>\n",
    "        <td style=\"border: 1px solid black;\">\n",
    "            people learn hate learn hate taught love</td>\n",
    "        <td style=\"border: 1px solid black;\">\n",
    "            love comes naturally human heart opposite</td>\n",
    "     </tr>\n",
    "</table>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stemming or lemmatization</h2>\n",
    "<ul>\n",
    "    <li>Optionally, apply <b>stemming</b> or <b>lemmatization</b> to the tokens. <!--terms.--></li>\n",
    "    <li>E.g. \"hating\" is replaced by \"hate\", \"comes\" is replaced by \"come\"</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running example</h2>\n",
    "<ul>\n",
    "    <li>What would the tweets look like after stemming?</li>\n",
    "    <li>What would they look like after lemmatization?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Count vectorization</h2>\n",
    "<ul>\n",
    "    <li>Each document becomes a vector, each token <!--term--> becomes a feature, feature-values are\n",
    "        <em>frequencies</em> (how many times that token <!--term--> appears in that document).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Running example</h2>\n",
    "<table>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th></th>\n",
    "        <th>background</th>\n",
    "        <th>born</th>\n",
    "        <th>color</th>\n",
    "        <th>comes</th>\n",
    "        <th>hate</th>\n",
    "        <th>hating</th>\n",
    "        <th>heart</th>\n",
    "        <th>human</th>\n",
    "        <th>learn</th>\n",
    "        <th>love</th>\n",
    "        <th>naturally</th>\n",
    "        <th>opposite</th>\n",
    "        <th>people</th>\n",
    "        <th>person</th>\n",
    "        <th>religion</th>\n",
    "        <th>skin</th>\n",
    "        <th>taught</th>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th>Tweet 0:</th>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th>Tweet 1:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>2</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>2</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th>Tweet 2:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "    </li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF vectorization</h2>\n",
    "<ul>\n",
    "    <li>Optionally, replace the frequencies by <b>tf-idf</b> scores.</li>\n",
    "    <li>tf-idf is a kind of standardization, but suitable for sparse data.</li>\n",
    "    <li>tf-idf scores penalise words that recur across multiple documents,\n",
    "        <ul>\n",
    "            <li>e.g. in emails, word such as \"hi\", \"best\", \"regards\", &hellip;</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>You can look up the formula, if you are interested. <!--For the formulae, see, e.g., <i>CS4611</i>.-->\n",
    "        <ul>\n",
    "            <li>Variants of the formula might: scale frequencies to avoid biases towards long documents \n",
    "                (not scikit-learn);\n",
    "                logarithmically scale frequencies (not default in scikit-learn);\n",
    "                add 1 to part of the formula to avoid division-by-zero (default in scikit-learn);\n",
    "                normalize the results (e.g. by default, scikit-learn divides by the $l_2$ norm)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running example</h2>\n",
    "<table>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th></th>\n",
    "        <th>background</th>\n",
    "        <th>born</th>\n",
    "        <th>color</th>\n",
    "        <th>comes</th>\n",
    "        <th>hate</th>\n",
    "        <th>hating</th>\n",
    "        <th>heart</th>\n",
    "        <th>human</th>\n",
    "        <th>learn</th>\n",
    "        <th>love</th>\n",
    "        <th>naturally</th>\n",
    "        <th>opposite</th>\n",
    "        <th>people</th>\n",
    "        <th>person</th>\n",
    "        <th>religion</th>\n",
    "        <th>skin</th>\n",
    "        <th>taught</th>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th>Tweet 0:</th>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0.38</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th>Tweet 1:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.61</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.61</td>\n",
    "        <td>0.23</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.31</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.31</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <th>Tweet 2:</th>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0</td>\n",
    "        <td>0.32</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0.42</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The dimension of these vectors</h2>\n",
    "<ul>\n",
    "    <li>Sparsity:\n",
    "        <ul>\n",
    "            <li>Here we have $n = 17$ features (columns). How many will there be in general?</li>\n",
    "            <li>Most of the feature-values are zero, hence the matrix is sparse. Why?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We have the curse of dimensionality again.\n",
    "        <ul>\n",
    "            <li>Reduce the number of features by:\n",
    "                <ul>\n",
    "                    <li>discarding tokens that appear in too few documents (<code>min_df</code> in scikit-learn);\n",
    "                    </li>\n",
    "                    <li>discarding tokens that appear in too many documents (<code>max_df</code>);</li>\n",
    "                    <li>keeping only the most frequent tokens (<code>max_features</code>).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Use dimensionality reduction:\n",
    "                <ul>\n",
    "                    <li>e.g. singular value decomposition (SVD) is suitable for bag-of-words, rather than PCA.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Observation about bag-of-words representations</h2>\n",
    "<ul>\n",
    "    <li>This representation is good for many applications in AI but it does have drawbacks too:\n",
    "        <ul>\n",
    "            <li>It loses all the information that English conveys through the order of words in sentences,\n",
    "                <ul>\n",
    "                    <li>e.g. \"People learn to hate\" and \"People hate to learn\" have very different meanings but\n",
    "                        end up with the same bag-of-words representation.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>It loses the information that English conveys using its stop-words, most notably negation,\n",
    "                <ul>\n",
    "                    <li>e.g. \"They hate religion\" and \"I do not hate religion\" will have the same bag-of-words\n",
    "                        representation.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This may not matter for some applications (e.g. spam detection) but will matter for\n",
    "        others (e.g. machine translation), for which you need a different representation.\n",
    "    </li>\n",
    "    <li>What other weaknesses does it have?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bag-of-Words in scikit-learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"No one is born hating another person because of the color of his skin or his background or his religion.\",\n",
    "    \"People must learn to hate, and if they can learn to hate, they can be taught to love.\",\n",
    "    \"For love comes more naturally to the human heart than its opposite.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Run the vectorizer\n",
    "vectorizer.fit(tweets)\n",
    "X = vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In the example below, we used a <code>CountVectorizer</code>.</li>\n",
    "    <li>It does tokenization:\n",
    "        <ul>\n",
    "            <li>By default, it converts to lowercase, it treats punctuation as spaces, and it treats two or more\n",
    "                consecutive characters as a word. Each word becomes a token <!--term--> (feature).\n",
    "            </li>\n",
    "        </ul>\n",
    "        <li>The example discards stop-words.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FYI here's the list of stop-words that scikit-learn uses.\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The <code>CountVectorizer</code> also, by default, discards any word that appears in every document.</li>\n",
    "    <li>It does not do stemming or lemmatization. scikit-learn doesn't have a stemmer, but does make it easy to call one, if you get one from another library, \n",
    "                e.g. NLTK.\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['background', 'born', 'color', 'comes', 'hate', 'hating', 'heart',\n",
       "       'human', 'learn', 'love', 'naturally', 'opposite', 'people',\n",
       "       'person', 'religion', 'skin', 'taught'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FYI, let's see the tokens (features) that it ends up with\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Finally, the <code>CountVectorizer</code> vectorizes, producing sparse matrices of word frequencies. \n",
    "        (There is an option to produce a binary representation, instead of frequencies.)\n",
    "    </li>\n",
    "    <li>We know we want this to be stored in an efficient sparse matrix, and scikit-learn takes care of this\n",
    "        'behind the scenes'. (Do not vectorize and then\n",
    "                convert back to Pandas DataFrames because, by default, DataFrames are not sparse data structures)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 18 stored elements and shape (3, 17)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 8)\t2\n",
      "  (1, 9)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 16)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "# We can look at the sparse array. The first number identifies the tweet (0, 1 or 2), \n",
    "# the second is which feature, and the last is the frequency\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize a new document\n",
    "new_document = \"Unsurprisingly, people hate to learn that their religion loves to hate.\"\n",
    "\n",
    "new_document_as_vector = vectorizer.transform([new_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 4 stored elements and shape (1, 17)>\n",
      "  Coords\tValues\n",
      "  (0, 4)\t2\n",
      "  (0, 8)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 14)\t1\n"
     ]
    }
   ],
   "source": [
    "# Notice how it ignores words that weren't in the original tweets, such as \"unsurprisingly\" and \"loves\"\n",
    "\n",
    "print(new_document_as_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In the example below, we use a <code>TfidfVectorizer</code> instead.</li>\n",
    "    <li>(By default, it normalizes the values using the $l_2$ norm.)</li> <!--, see CS46111).</li>-->\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Run the vectorizer\n",
    "vectorizer.fit(tweets)\n",
    "X = vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 18 stored elements and shape (3, 17)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t0.3779644730092272\n",
      "  (0, 1)\t0.3779644730092272\n",
      "  (0, 2)\t0.3779644730092272\n",
      "  (0, 5)\t0.3779644730092272\n",
      "  (0, 13)\t0.3779644730092272\n",
      "  (0, 14)\t0.3779644730092272\n",
      "  (0, 15)\t0.3779644730092272\n",
      "  (1, 4)\t0.6149219764307087\n",
      "  (1, 8)\t0.6149219764307087\n",
      "  (1, 9)\t0.2338320064840948\n",
      "  (1, 12)\t0.30746098821535434\n",
      "  (1, 16)\t0.30746098821535434\n",
      "  (2, 3)\t0.42339448341195934\n",
      "  (2, 6)\t0.42339448341195934\n",
      "  (2, 7)\t0.42339448341195934\n",
      "  (2, 9)\t0.3220024178194947\n",
      "  (2, 10)\t0.42339448341195934\n",
      "  (2, 11)\t0.42339448341195934\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize a new document\n",
    "new_document = \"Unsurprisingly, people hate to learn that their religion loves to hate.\"\n",
    "\n",
    "new_document_as_vector = vectorizer.transform([new_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4 stored elements and shape (1, 17)>\n",
      "  Coords\tValues\n",
      "  (0, 4)\t0.7559289460184544\n",
      "  (0, 8)\t0.3779644730092272\n",
      "  (0, 12)\t0.3779644730092272\n",
      "  (0, 14)\t0.3779644730092272\n"
     ]
    }
   ],
   "source": [
    "# Notice how it ignores words that weren't in the original tweets, such as \"unsurprisingly\" and \"loves\"\n",
    "\n",
    "print(new_document_as_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Similarity &amp; Distance for Bag-of-Words</h1>\n",
    "<ul>\n",
    "    <!--<li>For details and formulae, see CS4611.</li>-->\n",
    "    <li>Euclidean distance is not suitable.</li>\n",
    "    <li>Very common is <b>cosine similarity</b>, which gives values in $[0, 1]$, where 1 means 'identical'.</li>\n",
    "    <li>To get <b>cosine distance</b>, we can subtract from 1, so now 1 means 'completely different'.</li>\n",
    "    <li>The exact formulae differ depending on what is assumed about normalization.\n",
    "        <ul>\n",
    "            <li>If we assume the vectors have been normalized, then there is a simpler formula (Basically, just the dot product of the vectors).</li>\n",
    "            <li>If not, then the formula is more complicated (divide by the product of the $l_2$-norms).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Similarity &amp; distance for bag-of-words representation in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>The code below assumes that the vectors have already been normalized, e.g. produced\n",
    "        by <code>TfidfVectorizer</code>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x, xprime):\n",
    "    # Assumes x and  xprime are already normalized\n",
    "    # Converts from sparse matrices because np.dot does not work on them\n",
    "    return 1 - x.toarray().dot(xprime.toarray().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People must learn to hate, and if they can learn to hate, they can be taught to love.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So which of Barack Obama's tweets is most similar to our new document?\n",
    "tweets[np.argmin([cosine(new_document_as_vector, x) for x in X])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Case Study: A Classifier</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Stanford University researchers have taken 50,000 movie reviews from <a href=\"https://www.imdb.com/\">IMDB</a>,\n",
    "        labelled them as either positive or negative and <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">made them available</a>.\n",
    "    </li>\n",
    "    <li>I've taken the first 5,000 of them.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/dataset_5000_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just use holdout to keep things fast\n",
    "dev_df, test_df = train_test_split(df, train_size=0.8, stratify=df[\"sentiment\"], random_state=2)\n",
    "\n",
    "ss = ShuffleSplit(n_splits=1, train_size=0.75, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features but leave as a DataFrame\n",
    "dev_X = dev_df[\"review\"]\n",
    "test_X = test_df[\"review\"]\n",
    "\n",
    "# Target values, encoded and converted to a 1D numpy array\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"sentiment\"])\n",
    "dev_y = label_encoder.transform(dev_df[\"sentiment\"])\n",
    "test_y = label_encoder.transform(test_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>scikit-learn</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can just create a preprocessor with a CountVectorizer in a pipeline with logistic regression.\n",
    "logistic = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words='english', max_features=20000)),\n",
    "    (\"predictor\", LogisticRegression(max_iter=300))])\n",
    "\n",
    "# Get the validation error\n",
    "np.mean(cross_val_score(logistic, dev_X, dev_y, scoring=\"accuracy\", cv=ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of discarding features in the vectorizer, we can use TruncatedSVD for dimensionality reduction.\n",
    "logistic = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words='english', max_features=20000)),\n",
    "    (\"svd\", TruncatedSVD(n_components=100)),\n",
    "    (\"predictor\", LogisticRegression(max_iter=300))])\n",
    "\n",
    "# Get the validation error\n",
    "np.mean(cross_val_score(logistic, dev_X, dev_y, scoring=\"accuracy\", cv=ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'svd__n_components': 300,\n",
       "  'vectorizer__ngram_range': (1, 1),\n",
       "  'vectorizer__stop_words': 'english'},\n",
       " 0.824)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try some grid search\n",
    "logistic = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(max_features=20000)),\n",
    "    (\"svd\", TruncatedSVD()),\n",
    "    (\"predictor\", LogisticRegression(max_iter=300))])\n",
    "\n",
    "# Create a dictionary of hyperparameters for logistic regression\n",
    "logistic_param_grid = {\"vectorizer__stop_words\": [None, \"english\"],\n",
    "                       \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "                       \"svd__n_components\": [100, 200, 300]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "logistic_gs = GridSearchCV(logistic, logistic_param_grid, scoring=\"accuracy\", cv=ss, refit=True)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "logistic_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "logistic_gs.best_params_, logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'svd__n_components': 200,\n",
       "  'vectorizer__ngram_range': (1, 2),\n",
       "  'vectorizer__stop_words': None},\n",
       " 0.84)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And let's try it with a TF-IDF vectorizer\n",
    "tfidf_logistic = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(max_features=20000)),\n",
    "    (\"svd\", TruncatedSVD()),\n",
    "    (\"predictor\", LogisticRegression(max_iter=300))])\n",
    "\n",
    "# Create a dictionary of hyperparameters for logistic regression\n",
    "tfidf_logistic_param_grid = {\"vectorizer__stop_words\": [None, \"english\"],\n",
    "                             \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "                             \"svd__n_components\": [100, 200, 300]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "tfidf_logistic_gs = GridSearchCV(tfidf_logistic, tfidf_logistic_param_grid, scoring=\"accuracy\", cv=ss, refit=True)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "tfidf_logistic_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "tfidf_logistic_gs.best_params_, tfidf_logistic_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we re-train the winner on train+validation and test on the test set\n",
    "tfidf_logistic.set_params(**tfidf_logistic_gs.best_params_) \n",
    "tfidf_logistic.fit(dev_X, dev_y)\n",
    "accuracy_score(test_y, tfidf_logistic.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Keras</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Keras has a <code>TextVectorization</code> layer, which we can use for count vectorization (<code>output_mode=\"count\"</code>) or TF-IDF vectorization (<code>output_mode=\"tf-idf\"</code>).</li>\n",
    "    <li>It will do the tokenization, but it does not come with a method for stop-word removal (although you could write one).</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the count vectorization layer, and call adapt on the text-only dataset to create the vocabulary.\n",
    "vectorization_layer = TextVectorization(output_mode=\"count\", max_tokens=20000)\n",
    "vectorization_layer.adapt(convert_to_tensor(dev_df[\"review\"]))\n",
    "\n",
    "# Create and compile the model\n",
    "inputs = Input(shape=(1,), dtype=string, name=\"review\")\n",
    "x = vectorization_layer(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "count_model = Model(inputs, outputs)\n",
    "\n",
    "count_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30999d2b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model.fit(dev_X, dev_y, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709999918937683"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = count_model.evaluate(test_X, test_y, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TF-IDF vectorization layer, and call adapt on the text-only dataset to create the vocabulary.\n",
    "vectorization_layer = TextVectorization(output_mode=\"tf_idf\", max_tokens=20000)\n",
    "vectorization_layer.adapt(convert_to_tensor(dev_df[\"review\"]))\n",
    "\n",
    "# Create and compile the model\n",
    "inputs = Input(shape=(1,), dtype=string, name=\"review\")\n",
    "x = vectorization_layer(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "tfidf_model = Model(inputs, outputs)\n",
    "\n",
    "tfidf_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x308653710>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model.fit(dev_X, dev_y, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8640000224113464"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = tfidf_model.evaluate(test_X, test_y, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Great performances from my neural networks here, but we should probably plot a learning curve and\n",
    "        make sure that we are not over-fitting.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
